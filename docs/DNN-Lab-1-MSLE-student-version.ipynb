{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y676Rk9vA52c"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqdiMxZx-EoD"
      },
      "source": [
        "# Linear regression\n",
        "\n",
        "In this exercise, you will use linear regression to predict flat (apartment) prices. Training will be handled via gradient descent. We will:\n",
        "* have multiple features (i.e. variables used to make the prediction),\n",
        "* employ some basic feature engineering,\n",
        "* work with a non-standard loss function.\n",
        "\n",
        "Let's start by obtaining the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI9wFXv5-EoP",
        "outputId": "de1031e9-099f-4d21-9284-4e0c14b3a3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-06 00:21:20 URL:https://ucfae64b2f7daf7d60823240e21b.dl.dropboxusercontent.com/cd/0/inline/Cyp4kyjih446Ql_5B0ikFLNKJKx60XYBV9v8j6hPIiBFGB-Q-GGis3KpvrGcXST4s5UI4UA4Gz5nULw5JcX56Rbb-dpRjoomibKwxErPQ7mGK7ib-EPioxUoE-XH9YOkcEg/file?dl=1 [6211/6211] -> \"mieszkania.csv\" [1]\n",
            "2025-10-06 00:21:22 URL:https://ucbc2568eeddafee9b1fb5afd63f.dl.dropboxusercontent.com/cd/0/inline/CyrZWS1y7gqqmXERP0hdXf-ygcSE8BAwooKXuyuKkm65IGhfsDRHgMXU-CdfLOVJnTANSxJ1Bu3DmjpsHg67hX4ynUyEROPIdf4N90SMypEUpWeBGYLxEG_BW0W4b28xZug/file?dl=1 [6247/6247] -> \"mieszkania_test.csv\" [1]\n",
            "==> mieszkania.csv <==\n",
            "m2,dzielnica,ilość_sypialni,ilość_łazienek,rok_budowy,parking_podziemny,cena\n",
            "104,mokotowo,2,2,1940,1,780094\n",
            "43,ochotowo,1,1,1970,1,346912\n",
            "128,grodziskowo,3,2,1916,1,523466\n",
            "112,mokotowo,3,2,1920,1,830965\n",
            "149,mokotowo,3,3,1977,0,1090479\n",
            "80,ochotowo,2,2,1937,0,599060\n",
            "58,ochotowo,2,1,1922,0,463639\n",
            "23,ochotowo,1,1,1929,0,166785\n",
            "40,mokotowo,1,1,1973,0,318849\n",
            "\n",
            "==> mieszkania_test.csv <==\n",
            "m2,dzielnica,ilość_sypialni,ilość_łazienek,rok_budowy,parking_podziemny,cena\n",
            "71,wolowo,2,2,1912,1,322227\n",
            "45,mokotowo,1,1,1938,0,295878\n",
            "38,mokotowo,1,1,1999,1,306530\n",
            "70,ochotowo,2,2,1980,1,553641\n",
            "136,mokotowo,3,2,1939,1,985348\n",
            "128,wolowo,3,2,1983,1,695726\n",
            "23,grodziskowo,1,1,1975,0,99751\n",
            "117,mokotowo,3,2,1942,0,891261\n",
            "65,ochotowo,2,1,2002,1,536499\n"
          ]
        }
      ],
      "source": [
        "!wget --no-verbose -O mieszkania.csv https://www.dropbox.com/s/zey0gx91pna8irj/mieszkania.csv?dl=1\n",
        "!wget --no-verbose -O mieszkania_test.csv https://www.dropbox.com/s/dbrj6sbxb4ayqjz/mieszkania_test.csv?dl=1\n",
        "!head mieszkania.csv mieszkania_test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH8Ut02G-EoU"
      },
      "source": [
        "Each row in the data represents a separate flat. Our goal is to use the data from `mieszkania.csv` to create a model that can predict a flat's price (i.e. `cena`) given its features (i.e. `m2,dzielnica,ilosc_sypialni,...`).\n",
        "\n",
        "We should use only `mieszkania.csv` (dubbed the training dataset) to make our decisions and create the model. The (only) purpose of `mieszkania_test.csv` is to test our model on **unseen** data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "r_F0mDKdWNp4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NDArray = np.ndarray[Any, Any]\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "np.random.seed(357)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4eyhKJKom2s"
      },
      "source": [
        "## Loading and converting data\n",
        "\n",
        "Let's start by loading the data and showing the range of prices we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "NjwPnwzoWNs9"
      },
      "outputs": [],
      "source": [
        "def load(path: str) -> tuple[NDArray, NDArray]:\n",
        "    \"\"\"\n",
        "    Returns (x, y) where:\n",
        "    - x: input features, shape (n_apartments, n_features)\n",
        "    - y: price, shape (n_apartments,)\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(path)\n",
        "    y = data[\"cena\"].to_numpy()\n",
        "    x = data.loc[:, data.columns != \"cena\"].to_numpy()\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg-nV6_1WRk6",
        "outputId": "ba1f9ad7-486b-4e28-d620-7b3099c49d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200, 6) (200,)\n",
            "(200, 6) (200,)\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train = load(\"mieszkania.csv\")\n",
        "x_test, y_test = load(\"mieszkania_test.csv\")\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx5b_IpiZBNi",
        "outputId": "7e6ea03c-adc5-456c-d3c5-3c5598980509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "102572 1102309 507919.49\n"
          ]
        }
      ],
      "source": [
        "print(np.min(y_train), np.max(y_train), np.mean(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_oE2g2Yc9rt",
        "outputId": "0a634f01-3e0a-4432-dbef-0a7959586f53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[104, 'mokotowo', 2, 2, 1940, 1],\n",
              "       [43, 'ochotowo', 1, 1, 1970, 1],\n",
              "       [128, 'grodziskowo', 3, 2, 1916, 1]], dtype=object)"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js0uXZK-c_NA"
      },
      "source": [
        "We'll need to convert features to floats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "QVTpIDKuWRq6"
      },
      "outputs": [],
      "source": [
        "# Convert column 1 from str to (ordinal) int.\n",
        "# (One-hot encoding would be better, but ordinal is OK for today.)\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(x_train[:, 1])\n",
        "x_train[:, 1] = label_encoder.transform(x_train[:, 1])\n",
        "x_test[:, 1] = label_encoder.transform(x_test[:, 1])\n",
        "\n",
        "# Convert ints to float.\n",
        "x_train = x_train.astype(np.float64)\n",
        "x_test = x_test.astype(np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggvlLt-Tc3bX",
        "outputId": "4f606b8c-650a-4f8c-cd9a-3799e070795a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 104.,    1.,    2.,    2., 1940.,    1.],\n",
              "       [  43.,    2.,    1.,    1., 1970.,    1.],\n",
              "       [ 128.,    0.,    3.,    2., 1916.,    1.]])"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8QX6Ncr-EoW"
      },
      "source": [
        "## The loss and constant models\n",
        "\n",
        "Our predictions should minimize the so-called *mean squared logarithmic error*:\n",
        "$$\n",
        "MSLE = \\frac{1}{n} \\sum_{i=1}^n (\\log(1+y_i) - \\log(1+p_i))^2,\n",
        "$$\n",
        "where $y_i$ is the ground truth, and $p_i$ is our prediction.\n",
        "\n",
        "Let's implement the loss function first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "3tjQSlPgWXRN"
      },
      "outputs": [],
      "source": [
        "def mse(ys: NDArray, ps: NDArray) -> np.float64:\n",
        "    assert ys.shape == ps.shape\n",
        "    return np.mean((ys - ps) * (ys - ps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "JAiB8S9A-EoX"
      },
      "outputs": [],
      "source": [
        "def msle(ys: NDArray, ps: NDArray) -> np.float64:\n",
        "    assert ys.shape == ps.shape\n",
        "    return np.mean((np.log(ys+1)-np.log(ps+1))**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAmd2qLR-Eob"
      },
      "source": [
        "The simplest model is predicting the same constant for each instance. Test your implementation of msle against outputing the mean price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "b0RT7VW7-Eoc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(73810401517.75)"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_price = np.mean(y_train)\n",
        "\n",
        "mse(y_train, np.full_like(y_train, mean_price))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0zMmTiv-Eof"
      },
      "source": [
        "Recall that outputing the mean minimizes $MSE$. However, we're now dealing with $MSLE$.\n",
        "\n",
        "Think of a constant that should result in the lowest $MSLE$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "Q0Z4drNd-Eog"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.36488961221465716)"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cst = np.e**(np.mean([np.log(1+ys) for ys in y_train]))-1\n",
        "\n",
        "msle(y_train, np.full_like(y_train, cst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RdHlBw8-Eoi"
      },
      "source": [
        "## Linear regression (standard)\n",
        "\n",
        "Now, let's implement training of a standard linear regression model via gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "Vn97D3vd-Eoj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train MSE: 14778719338.96594\n",
            "test MSE: 18423067673.63713\n"
          ]
        }
      ],
      "source": [
        "def train(\n",
        "    x: NDArray, y: NDArray, alpha: float = 1e-7, n_iterations: int = 100000\n",
        ") -> tuple[NDArray, np.float64]:\n",
        "    \"\"\"Linear regression (which optimizes MSE). Returns (weights, bias).\"\"\"\n",
        "\n",
        "    # B is batch size (number of observations).\n",
        "    # F is number of (input) features.\n",
        "    B, F = x.shape\n",
        "    assert y.shape == (B,)\n",
        "\n",
        "    w = np.zeros(F, dtype=float)\n",
        "    b = 0.0\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        \n",
        "        y_pred = x @ w + b\n",
        "        err = y_pred - y\n",
        "\n",
        "        grad_w = (2/B)*(x.T @ err)\n",
        "        grad_b = (2/B)*np.sum(err)\n",
        "\n",
        "        w -= alpha*grad_w\n",
        "        b -= alpha*grad_b\n",
        "    \n",
        "    return (w, b)\n",
        "\n",
        "weights, bias = train(x_train, y_train)\n",
        "preds_train = x_train@weights + bias\n",
        "preds_test = x_test@weights + bias\n",
        "print(\"train MSE:\", mse(y_train, preds_train))\n",
        "print(\"test MSE:\", mse(y_test, preds_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfMn93ejteWt"
      },
      "source": [
        "## Linear regression (MSLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzrrOte5-Eol"
      },
      "source": [
        "Note that the loss function that the algorithms optimizes (i.e $MSE$) differs from $MSLE$. We've already seen that this may result in a suboptimal solution.\n",
        "\n",
        "How can you change the setting so that we optimze $MSLE$ instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVD8kWEJ-Eom"
      },
      "source": [
        "Hint:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Be lazy. We don't want to change the algorithm.\n",
        "Use the chain rule and previous computations to get formulas for the gradient.\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362,
          "referenced_widgets": [
            "854dd09ec9c14b23abe08b3439081a93",
            "382c8c66cd84479fb9f1027cb7017668",
            "ed99b0f1a0c348ebb364d855d71da507",
            "42b933092355453895883dcaee7d9abc",
            "9948365665274ca8adcf5e9f6e6c15df",
            "de121935829b478ba79640a20cd2e8f6",
            "38b2d7be86ce48e099fbdaf247789dc7",
            "aa9d653478904f7bba5e5ce9e3fff82b",
            "173083d5adbe4b4990afe5975909cb77",
            "779b0fbe29464ce7b29f2adac3c4951b",
            "b4bf97244dc24c46ac0cde4a3099916b"
          ]
        },
        "id": "MniIPMg8-Eom",
        "outputId": "4f7ad96e-a760-4f20-88c6-19ae85728aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train MSLE: 37.130688521448526\n",
            "test MSLE: 38.14321485039422\n"
          ]
        }
      ],
      "source": [
        "def train_msle(\n",
        "    x: NDArray, y: NDArray, alpha: float = 1e-7, n_iterations: int = 100000\n",
        ") -> tuple[NDArray, np.float64]:\n",
        "    \"\"\"Linear regression (which optimizes MSE). Returns (weights, bias).\"\"\"\n",
        "\n",
        "    # B is batch size (number of observations).\n",
        "    # F is number of (input) features.\n",
        "    B, F = x.shape\n",
        "    assert y.shape == (B,)\n",
        "\n",
        "    w = np.zeros(F, dtype=float)\n",
        "    b = 0.0\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        \n",
        "        y_pred = x @ w + b\n",
        "\n",
        "        y_pred_log = np.log(1+y_pred)\n",
        "        y_log = np.log(1+y)\n",
        "\n",
        "        err = y_pred_log - y_log\n",
        "\n",
        "        grad_w = (2/B)*x.T @ (err/(1+y_pred))\n",
        "        grad_b = (2/B)*np.sum(err/(1+y_pred))\n",
        "\n",
        "        w -= alpha*grad_w\n",
        "        b -= alpha*grad_b\n",
        "    \n",
        "    return (w, b)\n",
        "\n",
        "weights, bias = train_msle(x_train, y_train)\n",
        "preds_train = x_train@weights + bias\n",
        "preds_test = x_test@weights + bias\n",
        "print(\"train MSLE:\", msle(y_train, preds_train))\n",
        "print(\"test MSLE:\", msle(y_test, preds_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yfrbDbDrTns"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWrBkLu-Eoo"
      },
      "source": [
        "Without any feature engineering our model approximates the price as a linear combination of original features:\n",
        "$$\n",
        "\\text{price} \\approx w_1 \\cdot \\text{area} + w_2 \\cdot \\text{district} + \\dots.\n",
        "$$\n",
        "Let's now introduce some interactions between the variables. For instance, let's consider a following formula:\n",
        "$$\n",
        "\\text{price} \\approx w_1 \\cdot \\text{area} \\cdot \\text{avg. price in the district per sq. meter} + w_2 \\cdot \\dots + \\dots.\n",
        "$$\n",
        "Here, we model the price with far greater granularity, and we may expect to see more acurate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBM60E7t-Eop"
      },
      "source": [
        "Add some feature engineering to your model. Be sure to play with the data and not with the algorithm's code.\n",
        "\n",
        "Think how to make sure that your model is capable of capturing the $w_1 \\cdot \\text{area} \\cdot \\text{avg. price...}$ part, without actually computing the averages.\n",
        "\n",
        "Note that you may need to change the learning rate substantially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT5DziNC-Eoq"
      },
      "source": [
        "Hint:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Is having a binary encoding for each district and multiplying it by area enough?\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKOVCHNz-Eor"
      },
      "source": [
        "Hint 2:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Why not multiply everything together? I.e. (A,B,C) -> (AB,AC,BC).\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "IgfbFqwgEXdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 104.    1.    2. ...    4. 3880. 3880.]\n",
            " [  43.    2.    1. ...    1. 1970. 1970.]\n",
            " [ 128.    0.    3. ...    6. 5748. 3832.]\n",
            " ...\n",
            " [ 107.    0.    2. ...    0.    0.    0.]\n",
            " [ 117.    0.    3. ...    6. 5934. 3956.]\n",
            " [  56.    3.    2. ...    0.    0.    0.]]\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def feature_eng(x: NDArray) -> NDArray:\n",
        "    new_x = x\n",
        "\n",
        "    B, F = x.shape\n",
        "\n",
        "\n",
        "    for i in range(2, F-2):\n",
        "        combs = combinations(range(F), i)\n",
        "        for comb in combs:\n",
        "            new_column = np.ones((B, 1))\n",
        "            for index in comb:\n",
        "                new_column *= x[:, index][:, None]\n",
        "            new_x = np.concatenate((new_x, new_column), axis=1)\n",
        "    \n",
        "    return new_x\n",
        "\n",
        "new_x_train = feature_eng(x_train)\n",
        "new_x_test = feature_eng(x_test)\n",
        "print(new_x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "v2Lj1TPr-Eot"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train MSE: 13582282609.40337\n",
            "test MSE: 17336339863.193775\n",
            "train MSLE: 0.2548259000881113\n",
            "test MSLE: 0.29205031373719725\n"
          ]
        }
      ],
      "source": [
        "weights, bias = train(new_x_train, y_train, alpha=1e-12)\n",
        "preds_train = new_x_train@weights + bias\n",
        "preds_test = new_x_test@weights + bias\n",
        "print(\"train MSE:\", mse(y_train, preds_train))\n",
        "print(\"test MSE:\", mse(y_test, preds_test))\n",
        "\n",
        "weights, bias = train_msle(new_x_train, y_train)\n",
        "preds_train = new_x_train@weights + bias\n",
        "preds_test = new_x_test@weights + bias\n",
        "print(\"train MSLE:\", msle(y_train, preds_train))\n",
        "print(\"test MSLE:\", msle(y_test, preds_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdlipjGexuWl"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P1m1Bi1yEs_"
      },
      "source": [
        "In this exercise you will implement a validation pipeline: split the non-test set into train and validation sets and select the best model based on validation results.\n",
        "\n",
        "So far you tested your model against the training and test datasets. As you should observe, there's a gap between the results. By validating your model, you should be able to better anticipate the test time performance and compare different models and hyperparameters on datasets they are not over-fitted to.\n",
        "\n",
        "Implement the basic validation method, i.e. a random split. Test it with your model from Exercise MSLE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLvtN4Puxwu3",
        "outputId": "2324b9a7-4ee0-4e9b-8c0d-63998e4a4edc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200, 40, 200)"
            ]
          },
          "execution_count": 217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train_val, y_train_val = x_train, y_train\n",
        "x_test, y_test = x_test, y_test\n",
        "\n",
        "\n",
        "def random_split(\n",
        "    x: NDArray, y: NDArray, val_ratio: float = 0.2\n",
        ") -> tuple[tuple[NDArray, NDArray], tuple[NDArray, NDArray]]:\n",
        "    \"\"\"Returns (x_train, y_train), (x_val, y_val).\"\"\"\n",
        "\n",
        "    idxs = np.random.permutation(len(x))\n",
        "\n",
        "    val_idxs = idxs[:int(val_ratio*len(x))]\n",
        "    train_idxs = idxs[len(val_idxs):]\n",
        "\n",
        "    x_val, y_val = x[val_idxs], y[val_idxs]\n",
        "    x_train, y_train = x[train_idxs], y[train_idxs]\n",
        "\n",
        "    return (x_train, y_train), (x_val, y_val)\n",
        "\n",
        "\n",
        "\n",
        "(x_train_2, y_train_2), (x_val, y_val) = random_split(x_train_val, y_train_val)\n",
        "\n",
        "len(x_train), len(x_val), len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "CMc4r9oWHSZN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train MSLE: 0.26297032024709344\n",
            "train VAL: 0.3069762275142468\n",
            "test MSLE: 0.31179510411984646\n"
          ]
        }
      ],
      "source": [
        "(x_train_2, y_train_2), (x_val, y_val) = random_split(new_x_train, y_train)\n",
        "\n",
        "weights, bias = train_msle(x_train_2, y_train_2)\n",
        "preds_train = x_train_2@weights + bias\n",
        "preds_val = x_val@weights + bias\n",
        "preds_test = new_x_test@weights + bias\n",
        "print(\"train MSLE:\", msle(y_train_2, preds_train))\n",
        "print(\"train VAL:\", msle(y_val, preds_val))\n",
        "print(\"test MSLE:\", msle(y_test, preds_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LImWR9ki69T7"
      },
      "source": [
        "## Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWX_Soa25XJb"
      },
      "source": [
        "To make the random split validation reliable, a significant chunk of training data may be needed. To get over this problem, one may apply cross-validation.\n",
        "\n",
        "![alt-text](https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png)\n",
        "\n",
        "Let's now implement the method. Make sure that:\n",
        "* number of partitions is a parameter,\n",
        "* the method is not limited to `mieszkania.csv`,\n",
        "* the method is not limited to one specific model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma_OGKv80rln"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "# TODO: Implement cross-validation #\n",
        "####################################\n",
        "def kfold(x: NDArray, y: NDArray, n_folds: int = 5, shuffle: bool = False) -> list[float]:\n",
        "    \"\"\"Returns losses for each fold.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "losses = kfold(x_train_val, y_train_val, n_folds=3, shuffle=False)\n",
        "print(f\"k-fold loss: {np.mean(losses):.4f} +- {np.std(losses):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7h-hJl7Q_3"
      },
      "source": [
        "## Investigating input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFeQiTQc7Rid"
      },
      "source": [
        "Recall that sometimes validation may be tricky, e.g. significant class imbalance, having a small number of subjects, geographically clustered instances...\n",
        "\n",
        "What could in theory go wrong here with random, unstratified partitions? Think about potential solutions and investigate the data in order to check whether these problems arise here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JQKK6wZ7SAi"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# TODO: Investigate the data #\n",
        "##############################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "GSN",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
