{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Multitask Learning for Geometric Shape Classification and Counting**\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "In this project, you will design, implement, and evaluate a **multitask neural network** that performs **two tasks simultaneously**:\n",
        "\n",
        "1. **Classification** – identify which pair of geometric shape types appears in a 28×28 binary image (135 possible configurations).\n",
        "2. **Regression** – predict how many shapes of each type are present (6 regression targets).\n",
        "\n",
        "This project focuses on **multi-task learning**, i.e., using one shared model to learn several related tasks at once. You will compare how adding an auxiliary task affects performance and training dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset\n",
        "\n",
        "You will use the **Geometric Shape Numbers (GSN)** dataset:\n",
        "\n",
        "```bash\n",
        "!wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip\n",
        "```\n",
        "\n",
        "This will create a directory `data/` containing:\n",
        "\n",
        "* **10,000 images** (28×28x1, grayscale)\n",
        "* **labels.csv** – counts of each of six shape types per image\n",
        "\n",
        "Each image contains exactly **two types** of geometric figures (out of six) and **10 shapes total**.\n",
        "\n",
        "**Shape classes:**\n",
        "\n",
        "| Index | Shape type     |\n",
        "| ----: | -------------- |\n",
        "|     0 | square         |\n",
        "|     1 | circle         |\n",
        "|     2 | triangle up    |\n",
        "|     3 | triangle right |\n",
        "|     4 | triangle down  |\n",
        "|     5 | triangle left  |\n",
        "\n",
        "Example row from `labels.csv`:\n",
        "\n",
        "```\n",
        "name,squares,circles,up,right,down,left\n",
        "img_00000.png,0,0,0,4,0,6\n",
        "```\n",
        "\n",
        "Here, the image contains **4 right-pointing triangles** and **6 left-pointing triangles**.\n",
        "\n",
        "**Split:**\n",
        "\n",
        "* Training: first 9,000 samples\n",
        "* Validation: last 1,000 samples\n",
        "\n",
        "Examples:\n",
        "![example.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Tasks and objectives\n",
        "\n",
        "You must design a **multitask deep learning system** that:\n",
        "\n",
        "1. **Classifies** each image into one of **135 possible configurations**, representing:\n",
        "\n",
        "   * which **two shape classes** appear, and\n",
        "   * how their counts (1–9) sum to 10.\n",
        "\n",
        "   → Example: \"3 circles + 7 squares\" is one configuration class.\n",
        "\n",
        "2. **Regresses** the number of shapes of each type (a 6-dimensional real-valued output).\n",
        "\n",
        "3. Combines both objectives in a **joint loss** function (Hint: losses are implemented in PyTorch):\n",
        "\n",
        "\n",
        "$$ Loss = \\text{NLLLoss(classification)} + \\lambda_{\\text{cnt}} \\cdot \\text{SmoothL1Loss(regression)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model requirements\n",
        "\n",
        "### Architecture constraints\n",
        "\n",
        "You must use **exactly this feature extractor (backbone)**:\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        ")\n",
        "```\n",
        "\n",
        "Then add **two separate heads**:\n",
        "\n",
        "* `head_cls`: outputs log-probabilities for 135 classes\n",
        "* `head_cnt`: outputs 6 regression values (counts)\n",
        "\n",
        "The model must return two outputs: `(log_probs, counts)`.\n",
        "\n",
        "You may add dropout or batch normalization inside the heads, **but you must not modify the backbone**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Training setup\n",
        "\n",
        "* Optimizer: **Adam**, learning rate = 1e-3\n",
        "* Epochs: up to **100** (use **early stopping**)\n",
        "* Batch sizes: **64** (train), **1000** (validation)\n",
        "* Device: GPU allowed for Notebook, but your **final code must run on GPU within ~30 minutes**\n",
        "* Random seed: set `torch.manual_seed(1)` for reproducibility\n",
        "* Split: **exactly 9,000 train / 1,000 validation**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Data preprocessing and augmentation\n",
        "\n",
        "You must implement a **PyTorch `Dataset` class** that:\n",
        "\n",
        "* Reads `labels.csv`\n",
        "* Loads the corresponding image (from `data/`)\n",
        "* Returns both:\n",
        "  * the image (as a tensor)\n",
        "  * the labels (counts for 6 shapes)\n",
        "* Optionally applies transformations\n",
        "\n",
        "### Required augmentations\n",
        "\n",
        "You must implement **at least three** of the following:\n",
        "\n",
        "1. Random horizontal flip\n",
        "2. Random vertical flip\n",
        "3. Random 90° rotation (must correctly rotate orientation labels: up → right → down → left)\n",
        "4. Random brightness/contrast (mild)\n",
        "5. Gaussian noise\n",
        "6. Random erasing (small areas only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation metrics\n",
        "\n",
        "Implement and report the following metrics on the validation set:\n",
        "\n",
        "### (a) **Classification (135-way)**\n",
        "\n",
        "* Top-1 accuracy\n",
        "* Macro F1-score\n",
        "* Per-pair accuracy (aggregate by unordered shape pair, e.g. {circle, up})\n",
        "\n",
        "### (b) **Regression (6-D counts)**\n",
        "\n",
        "* RMSE per class and overall\n",
        "* MAE per class and overall\n",
        "\n",
        "Also plot:\n",
        "\n",
        "* Training and validation losses\n",
        "* Validation accuracy and RMSE over epochs\n",
        "\n",
        "**Important**: This task is not about finding the best architecture; we expect at least 50% accuracy, but achieving results higher than that will not affect the grade for the assignment**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Experiments and analysis\n",
        "\n",
        "You must train and compare **three model settings**:\n",
        "\n",
        "| Setting | Description                                      |\n",
        "| :------ | :----------------------------------------------- |\n",
        "| 1       | **Classification-only:** λ_cnt = 0               |\n",
        "| 2       | **Regression-only:** classification loss ignored |\n",
        "| 3       | **Multitask:** λ_cnt = with your choose          |\n",
        "\n",
        "For each experiment:\n",
        "\n",
        "* Train until early stopping\n",
        "* Record loss, accuracy, RMSE, and runtime\n",
        "* Compare results and explain how λ influences learning\n",
        "* Discuss whether multitask learning improves the main tasks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Final deliverables\n",
        "\n",
        "You must submit .zip project with:\n",
        "\n",
        "1. **Code** (`.ipynb` or `.py`) that:\n",
        "\n",
        "   * Downloads and extracts the dataset\n",
        "   * Defines dataset, dataloaders, model, loss, training loop, evaluation, and plotting\n",
        "   * Can run start-to-end without interaction, and finishes within 30 minutes on Colab T4 GPUs\n",
        "   * Includes three experiment configurations\n",
        "\n",
        "2. **Report (2–4 pages, PDF)** including:\n",
        "   * Section on (EDA) Exploratory Data Analysis in your report: no more than 3 graphs or tables describing the data set.\n",
        "   * Model architecture\n",
        "   * Description and justification of augmentations\n",
        "   * Results table (loss, accuracy, RMSE for all runs)\n",
        "   * Learning curves\n",
        "   * Discussion on multitask effects\n",
        "\n",
        "3. **README.md**:\n",
        "\n",
        "   * Link to Colab version of task for fast replication.\n",
        "   * Approximate runtime and resource requirements\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Grading rubric\n",
        "\n",
        "Component\tDescription\tPoints\n",
        "1. Implementation correctness\tCorrect use of the fixed backbone, two-headed model, and proper training loop (classification + regression).\t30%\n",
        "2. Data & augmentations\tProper dataset loading, preprocessing, and at least three augmentations with brief justification.\t20%\n",
        "3. Evaluation & experiments\tCorrect computation of metrics (accuracy, F1, RMSE) and completion of all three λ configurations (λ=0, regression-only, your choice λ).\t30%\n",
        "4. Report & analysis\n",
        "A clear separation of concerns (e.g. headers in notebooks, modules in code) and concise 2–4 page report with results tables, learning curves, confusion matrix, and short discussion on multitask effects and error examples.\n",
        "20%\n",
        "\n",
        "###### Readability and modularity will be considered within each grading component. Clear structure (headers in notebooks, docstrings, modular code) significantly improves evaluation speed. Emphasize using clear headers to help reviewers navigate efficiently.\n",
        "---"
      ],
      "metadata": {
        "id": "_NvRrg8YvTPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmggC-uvcGR",
        "outputId": "741fb25f-d49d-41ba-8429-9447571dbe80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-10 12:39:11--  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip [following]\n",
            "--2025-11-10 12:39:11--  https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5544261 (5.3M) [application/zip]\n",
            "Saving to: ‘data_gsn.zip’\n",
            "\n",
            "data_gsn.zip        100%[===================>]   5.29M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-10 12:39:12 (102 MB/s) - ‘data_gsn.zip’ saved [5544261/5544261]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating labels for classification problem\n",
        "\n",
        "There are 135 combinations. This comes from:\n",
        "$$\\binom{6}{2}\\cdot 9 = 15\\cdot9 = 135$$\n",
        "\n",
        "We propose the following labeling: <br>\n",
        "For counting label $$[a_0, a_1, a_2, a_3, a_4, a_5]$$ with the only non-zero elements $a_i$ and $a_j$, where $i<j$ we will use a label equal to:\n",
        "$$ f(i, j)\\cdot9+a_i-1$$\n",
        "Where $f(g, h)$, where $g<h$, is a simple indexing function: <br>\n",
        "$$f(g,h) = \\binom{h}{2}+g$$ where $0\\leq g<h\\leq5$. This provides us with unique labels from $0$ to $134$"
      ],
      "metadata": {
        "id": "clsXRR2XUjf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math as mth\n",
        "\n",
        "def cnt_to_cls_idx(cnt_label):\n",
        "  a_1, i_1, i_2 = 0, 0, 0\n",
        "  for i, cnt in enumerate(cnt_label):\n",
        "    if cnt > 0:\n",
        "      if not a_1:\n",
        "        a_1 = cnt\n",
        "        i_1 = i\n",
        "      else:\n",
        "        i_2 = i\n",
        "        break\n",
        "  cls_idx = (mth.comb(i_2, 2)+i_1)*9+a_1-1\n",
        "  return cls_idx\n",
        "\n",
        "def cls_idx_to_cnt(cls_idx):\n",
        "    if not (0 <= cls_idx <= 134):\n",
        "        raise ValueError(\"cls_idx must be in [0, 134] for 6 labels and counts 1..9\")\n",
        "\n",
        "    a_1 = (cls_idx % 9) + 1\n",
        "    pair_idx = cls_idx // 9\n",
        "    i_2 = 1\n",
        "    while mth.comb(i_2 + 1, 2) <= pair_idx:\n",
        "        i_2 += 1\n",
        "    i_1 = pair_idx - mth.comb(i_2, 2)\n",
        "\n",
        "    cnt_label = [0]*6\n",
        "    cnt_label[i_1] = a_1\n",
        "    cnt_label[i_2] = 10 - a_1\n",
        "    return cnt_label\n",
        "\n",
        "\n",
        "# Test the labeling and create O(1) mapping\n",
        "labels = set()\n",
        "for a1 in range(1, 10):\n",
        "  for idx1 in range(6):\n",
        "    for idx2 in range(idx1+1, 6):\n",
        "      label_cnt = [0]*6\n",
        "      label_cnt[idx1] = a1\n",
        "      label_cnt[idx2] = 10-a1\n",
        "      label = cnt_to_cls_idx(label_cnt)\n",
        "      if label in labels:\n",
        "        print(f\"Not ok {label}\")\n",
        "      if cls_idx_to_cnt(label) != label_cnt:\n",
        "        print(\"Not ok\")\n",
        "      labels.add(label)\n",
        "\n",
        "if len(labels) == 135:\n",
        "  print(\"Ok\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BSqeLuQ1Uqpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding new labels"
      ],
      "metadata": {
        "id": "xs6a3_uSYYNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CSV_PATH = \"./data/labels.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "label_cols = df.columns[1:]\n",
        "n_images = len(df)\n",
        "\n",
        "class_indices = []\n",
        "for i in range(n_images):\n",
        "    cnt_label = df[label_cols].iloc[i].values\n",
        "    class_idx = cnt_to_cls_idx(cnt_label)\n",
        "    class_indices.append(class_idx)\n",
        "\n",
        "df[\"class_idx\"] = class_indices\n"
      ],
      "metadata": {
        "id": "eAGB4xBvYcg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Data\n"
      ],
      "metadata": {
        "id": "rhOb2Wyoyfer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "cnt_label_cols = df.columns[1:7]\n",
        "n_images = len(df)\n",
        "\n",
        "\n",
        "# --- COUNT LABELS ANALYSIS ---\n",
        "total_shapes     = df[cnt_label_cols].sum().sum()\n",
        "shape_totals     = df[cnt_label_cols].sum()\n",
        "shape_percentage = 100 * shape_totals / total_shapes\n",
        "\n",
        "corr = df[cnt_label_cols].corr(numeric_only=True)\n",
        "nonzero_means = df[cnt_label_cols].replace(0, np.nan).mean()\n",
        "presence_percentage = 100 * (df[cnt_label_cols] > 0).sum() / n_images\n",
        "\n",
        "# Per-image constraints\n",
        "nonzero_per_image = (df[cnt_label_cols] > 0).sum(axis=1)\n",
        "sum_per_image     = df[cnt_label_cols].sum(axis=1)\n",
        "\n",
        "prop_exact_two_nonzero = (nonzero_per_image == 2).mean() * 100\n",
        "prop_sum_eq_10         = (sum_per_image == 10).mean() * 100\n",
        "prop_both_constraints  = ((nonzero_per_image == 2) & (sum_per_image == 10)).mean() * 100\n",
        "\n",
        "# --- CLASS INDEX ANALYSIS (robust) ---\n",
        "n_classes = 135\n",
        "\n",
        "# counts for all classes [0..n_classes-1], missing -> 0\n",
        "class_counts_raw = df[\"class_idx\"].value_counts()          # counts only for seen classes\n",
        "class_counts_full = class_counts_raw.reindex(range(n_classes), fill_value=0)\n",
        "\n",
        "# summary table: count + % of images containing that class\n",
        "class_presence_pct = 100 * class_counts_full / n_images\n",
        "class_summary = pd.DataFrame({\n",
        "    \"class_id\": range(n_classes),\n",
        "    \"count\": class_counts_full.values,\n",
        "    \"pct_of_images\": class_presence_pct.values\n",
        "})\n",
        "\n",
        "# stats\n",
        "num_used_classes = int((class_counts_full > 0).sum())\n",
        "pct_classes_used = 100 * num_used_classes / n_classes\n",
        "unused_classes = class_summary.loc[class_summary[\"count\"] == 0, \"class_id\"].tolist()\n",
        "\n",
        "# top / bottom 10 (including unseen)\n",
        "top10_classes = class_summary.sort_values(\"count\", ascending=False).head(10)\n",
        "least10_present = (\n",
        "    class_summary[class_summary[\"count\"] > 0]\n",
        "    .sort_values([\"count\", \"class_id\"], ascending=[True, True])\n",
        "    .head(10)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "# --- PRINT SUMMARY ---\n",
        "print(\"=== DATA OVERVIEW ===\")\n",
        "print(f\"Images: {n_images}\")\n",
        "print(f\"Count label columns: {list(label_cols)}\")\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "print(\"\\n=== COUNT LABELS SUMMARY ===\")\n",
        "print(f\"Total sum of all counts: {int(total_shapes)}\")\n",
        "print(\"\\nPer-label totals:\")\n",
        "print(shape_totals)\n",
        "print(\"\\nPer-label percentage of total (%):\")\n",
        "print(shape_percentage.round(2))\n",
        "print(\"\\nPresence percentage per label (%% of images with >0):\")\n",
        "print(presence_percentage.round(2))\n",
        "print(\"\\nMean of non-zero values per label:\")\n",
        "print(nonzero_means.round(3))\n",
        "\n",
        "print(\"\\n=== CONSTRAINT CHECKS ===\")\n",
        "print(f\"Images with exactly 2 non-zero count labels: {prop_exact_two_nonzero:.2f}%\")\n",
        "print(f\"Images with sum(counts) == 10: {prop_sum_eq_10:.2f}%\")\n",
        "print(f\"Images satisfying both: {prop_both_constraints:.2f}%\")\n",
        "\n",
        "print(\"\\nCorrelation matrix between count labels:\")\n",
        "print(corr.round(3))\n",
        "\n",
        "print(f\"Classes used: {num_used_classes}/{n_classes} ({pct_classes_used:.2f}%)\")\n",
        "print(\"Top 10 classes (id, count, % of images):\")\n",
        "print(top10_classes[[\"class_id\", \"count\", \"pct_of_images\"]].to_string(index=False))\n",
        "\n",
        "print(\"\\nLeast 10 classes (id, count, % of images):\")\n",
        "print(least10_present[[\"class_id\", \"count\", \"pct_of_images\"]].to_string(index=False))\n",
        "print(\"Unused common classes:\")\n",
        "print(unused_classes)\n"
      ],
      "metadata": {
        "id": "KbjAlQiVyjY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "The dataset is well balanced: all shapes appear with similar frequency, there are no strong correlations between shape types, and whenever a shape is present its average count is close to 5 out of 10. Based on this, not much data augmentation is needed."
      ],
      "metadata": {
        "id": "eVQLy-7S7_WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class\n",
        "Each transform is applied independently with a fixed probability."
      ],
      "metadata": {
        "id": "L4QwKRS1uwnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ake3FpnUpxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as F\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "class MultitaskDataset(Dataset):\n",
        "\n",
        "  def __init__(self, root: str, transforms = None, transform_prob: float = 0.3):\n",
        "    self.root = Path(root)\n",
        "    self.df = pd.read_csv(root)\n",
        "    if transforms is None:\n",
        "      transforms = []\n",
        "    self.transforms = transforms\n",
        "    self.transform_prob = transform_prob\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.root.parent / self.df.iloc[idx, 0]\n",
        "    image = Image.open(img_path)\n",
        "    label_cnt = self.df.iloc[idx, 1:].tolist()\n",
        "\n",
        "\n",
        "\n",
        "    for transform in self.transforms:\n",
        "      if torch.rand(1) < self.transform_prob:\n",
        "        image, labels_cnt = transform(image, label_cnt)\n",
        "\n",
        "\n",
        "    cls_idx = cnt_to_cls_idx(label_cnt)\n",
        "    label_cls = [(i == cls_idx) for i in range(135)]\n",
        "    image = F.to_tensor(image)\n",
        "\n",
        "    return image, torch.tensor(label_cnt, dtype=torch.float32), torch.tensor(label_cls, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def horizontal_flip(image, labels_cnt):\n",
        "  labels_cnt[3], labels_cnt[5] = labels_cnt[5], labels_cnt[3]\n",
        "  return image.transpose(Image.FLIP_LEFT_RIGHT), labels_cnt\n",
        "\n",
        "\n",
        "def vertical_flip(image, labels_cnt):\n",
        "  labels_cnt[2], labels_cnt[4] = labels_cnt[4], labels_cnt[2]\n",
        "  return image.transpose(Image.FLIP_TOP_BOTTOM), labels_cnt\n",
        "\n",
        "def rotation(image, labels_cnt):\n",
        "  labels_cnt = [labels_cnt[0], labels_cnt[1], labels_cnt[5], labels_cnt[2], labels_cnt[3], labels_cnt[4]]\n",
        "  return image.rotate(90), labels_cnt\n",
        "\n",
        "dataset = MultitaskDataset(\n",
        "    root = './data/labels.csv',\n",
        "    transforms=[horizontal_flip, vertical_flip, rotation],\n",
        "    transform_prob=0.1\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "Noooak7Y18Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the data"
      ],
      "metadata": {
        "id": "vE1fwqZovBc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import pandas as pd\n",
        "\n",
        "g = torch.Generator().manual_seed(1)\n",
        "\n",
        "train_base = MultitaskDataset(\n",
        "    root=\"./data/labels.csv\",\n",
        "    transforms=[horizontal_flip, vertical_flip, rotation],\n",
        "    transform_prob=0.3\n",
        ")\n",
        "\n",
        "test_base = MultitaskDataset(\n",
        "    root=\"./data/labels.csv\",\n",
        "    transforms=[],\n",
        ")\n",
        "\n",
        "n = len(train_base)\n",
        "split = int(0.9 * n)\n",
        "\n",
        "train_idx = [i for i in range(split)]\n",
        "test_idx  = [i for i in range(split,n)]\n",
        "\n",
        "assert (len(train_idx), len(test_idx)) == (9000, 1000)\n",
        "\n",
        "train_ds = Subset(train_base, train_idx)\n",
        "test_ds  = Subset(test_base,  test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "uDskVAXfvkFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network structure\n"
      ],
      "metadata": {
        "id": "_ObxxKDb9YDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        "        )\n",
        "        self.head_cls = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 135)\n",
        "        )\n",
        "        self.head_cnt = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 6)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        logits = self.head_cls(feat)\n",
        "        log_probs = torchVTF.log_softmax(logits, dim=1)\n",
        "        counts = self.head_cnt(feat)\n",
        "        return log_probs, counts\n"
      ],
      "metadata": {
        "id": "Ps2PbavO95CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*## Training\n"
      ],
      "metadata": {
        "id": "D32dQyZbHAhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <small>Helper class: TrainingVisualizer</small>\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "class TrainingVisualizer:\n",
        "    def __init__(self, log_interval: int = 10):\n",
        "        self.log_interval = log_interval\n",
        "        self.train_loss_fig = self.init_line_plot(\n",
        "            title=\"Training loss\", xaxis_title=\"Step\"\n",
        "        )\n",
        "        self.grad_to_weight_fig = self.init_line_plot(\n",
        "            title=\"Gradient standard deviation to weight standard deviation ratio at 1st layer\",\n",
        "            xaxis_title=\"Step\",\n",
        "            yaxis_title=\"Gradient to weight ratio (log scale)\",\n",
        "            yaxis_type=\"log\",\n",
        "        )\n",
        "        self.test_acc_fig = self.init_line_plot(\n",
        "            title=\"Test accuracy\", x=[], xaxis_title=\"Epoch\", mode=\"lines+markers\"\n",
        "        )\n",
        "\n",
        "        # Parameters related to current tracked model and its training\n",
        "        self.first_linear_layer = None\n",
        "        self.lr = None\n",
        "        self.trace_idx = -1\n",
        "\n",
        "    def init_line_plot(\n",
        "        self,\n",
        "        title: str,\n",
        "        x=None,\n",
        "        xaxis_title: str | None = None,\n",
        "        yaxis_title: str | None = None,\n",
        "        yaxis_type: str = \"linear\",\n",
        "        mode: str = \"lines\",\n",
        "    ) -> go.FigureWidget:\n",
        "        fig = go.Figure()\n",
        "        fig.update_layout(\n",
        "            title=title,\n",
        "            title_x=0.5,\n",
        "            xaxis_title=xaxis_title,\n",
        "            yaxis_title=yaxis_title,\n",
        "            height=400,\n",
        "            width=1500,\n",
        "            margin=dict(b=10, t=60),\n",
        "        )\n",
        "        fig.update_yaxes(type=yaxis_type)\n",
        "        # We cannot add new traces dynamically because Colab has a problem with widgets\n",
        "        # from plotly (traces added dynamically are rendered twice).\n",
        "        # As an ugly workaround we create a lot of empty traces and update them later\n",
        "        # with actual data. Empty traces are not plotted.\n",
        "        for _ in range(25):\n",
        "            fig.add_trace(go.Scatter(x=x, y=[], showlegend=True, mode=mode))\n",
        "\n",
        "        fig_widget = go.FigureWidget(fig)\n",
        "        display(fig_widget)\n",
        "        return fig_widget\n",
        "\n",
        "    def track_model(\n",
        "        self, model: torch.nn.Module, optimizer: torch.optim.Optimizer, lr: float\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Start tracking training metrics for a new model.\n",
        "        \"\"\"\n",
        "\n",
        "        for field in model.__dict__[\"_modules\"].values():\n",
        "            if isinstance(field, nn.Linear):\n",
        "                self.first_linear_layer = field\n",
        "                break\n",
        "            elif isinstance(field, nn.ModuleList):\n",
        "                self.first_linear_layer = field[0]\n",
        "                break\n",
        "\n",
        "        self.lr = lr\n",
        "        self.trace_idx += 1\n",
        "\n",
        "        optim_name = type(optimizer).__name__\n",
        "        self.train_loss_fig.data[self.trace_idx].name = f\"{optim_name}, {lr}\"\n",
        "        self.grad_to_weight_fig.data[self.trace_idx].name = f\"{optim_name}, {lr}\"\n",
        "        self.test_acc_fig.data[self.trace_idx].name = f\"{optim_name}, {lr}\"\n",
        "\n",
        "    def plot_gradients_and_loss(self, batch_idx: int, loss: float) -> None:\n",
        "        if batch_idx % self.log_interval == 0:\n",
        "            self.train_loss_fig.data[self.trace_idx].y += (loss,)\n",
        "\n",
        "            layer = self.first_linear_layer\n",
        "            grad_to_weight_ratio = (\n",
        "                self.lr * layer.weight.grad.std() / layer.weight.std()\n",
        "            ).item()\n",
        "\n",
        "            self.grad_to_weight_fig.data[self.trace_idx].y += (grad_to_weight_ratio,)\n",
        "\n",
        "    def plot_accuracy(self, epoch: int, accuracy: float) -> None:\n",
        "        self.test_acc_fig.data[self.trace_idx].x += (epoch,)\n",
        "        self.test_acc_fig.data[self.trace_idx].y += (accuracy,)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ria9VZF0G6k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    device: torch.device,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epoch: int,\n",
        "    log_interval: int,\n",
        "    visualizer: TrainingVisualizer,\n",
        "    verbose: bool = False,\n",
        ") -> None:\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target_cls, target_cnt = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        log_probs, counts = model(data)\n",
        "\n",
        "        loss_cls = F.nll_loss(log_probs, target)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nM1ILMQ07fTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "epochs ="
      ],
      "metadata": {
        "id": "GnzDL1pSAot5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}