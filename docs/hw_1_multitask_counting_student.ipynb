{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Multitask Learning for Geometric Shape Classification and Counting**\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "In this project, you will design, implement, and evaluate a **multitask neural network** that performs **two tasks simultaneously**:\n",
        "\n",
        "1. **Classification** – identify which pair of geometric shape types appears in a 28×28 binary image (135 possible configurations).\n",
        "2. **Regression** – predict how many shapes of each type are present (6 regression targets).\n",
        "\n",
        "This project focuses on **multi-task learning**, i.e., using one shared model to learn several related tasks at once. You will compare how adding an auxiliary task affects performance and training dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset\n",
        "\n",
        "You will use the **Geometric Shape Numbers (GSN)** dataset:\n",
        "\n",
        "```bash\n",
        "!wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip\n",
        "```\n",
        "\n",
        "This will create a directory `data/` containing:\n",
        "\n",
        "* **10,000 images** (28×28x1, grayscale)\n",
        "* **labels.csv** – counts of each of six shape types per image\n",
        "\n",
        "Each image contains exactly **two types** of geometric figures (out of six) and **10 shapes total**.\n",
        "\n",
        "**Shape classes:**\n",
        "\n",
        "| Index | Shape type     |\n",
        "| ----: | -------------- |\n",
        "|     0 | square         |\n",
        "|     1 | circle         |\n",
        "|     2 | triangle up    |\n",
        "|     3 | triangle right |\n",
        "|     4 | triangle down  |\n",
        "|     5 | triangle left  |\n",
        "\n",
        "Example row from `labels.csv`:\n",
        "\n",
        "```\n",
        "name,squares,circles,up,right,down,left\n",
        "img_00000.png,0,0,0,4,0,6\n",
        "```\n",
        "\n",
        "Here, the image contains **4 right-pointing triangles** and **6 left-pointing triangles**.\n",
        "\n",
        "**Split:**\n",
        "\n",
        "* Training: first 9,000 samples\n",
        "* Validation: last 1,000 samples\n",
        "\n",
        "Examples:\n",
        "![example.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Tasks and objectives\n",
        "\n",
        "You must design a **multitask deep learning system** that:\n",
        "\n",
        "1. **Classifies** each image into one of **135 possible configurations**, representing:\n",
        "\n",
        "   * which **two shape classes** appear, and\n",
        "   * how their counts (1–9) sum to 10.\n",
        "\n",
        "   → Example: \"3 circles + 7 squares\" is one configuration class.\n",
        "\n",
        "2. **Regresses** the number of shapes of each type (a 6-dimensional real-valued output).\n",
        "\n",
        "3. Combines both objectives in a **joint loss** function (Hint: losses are implemented in PyTorch):\n",
        "\n",
        "\n",
        "$$ Loss = \\text{NLLLoss(classification)} + \\lambda_{\\text{cnt}} \\cdot \\text{SmoothL1Loss(regression)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model requirements\n",
        "\n",
        "### Architecture constraints\n",
        "\n",
        "You must use **exactly this feature extractor (backbone)**:\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        ")\n",
        "```\n",
        "\n",
        "Then add **two separate heads**:\n",
        "\n",
        "* `head_cls`: outputs log-probabilities for 135 classes\n",
        "* `head_cnt`: outputs 6 regression values (counts)\n",
        "\n",
        "The model must return two outputs: `(log_probs, counts)`.\n",
        "\n",
        "You may add dropout or batch normalization inside the heads, **but you must not modify the backbone**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Training setup\n",
        "\n",
        "* Optimizer: **Adam**, learning rate = 1e-3\n",
        "* Epochs: up to **100** (use **early stopping**)\n",
        "* Batch sizes: **64** (train), **1000** (validation)\n",
        "* Device: GPU allowed for Notebook, but your **final code must run on GPU within ~30 minutes**\n",
        "* Random seed: set `torch.manual_seed(1)` for reproducibility\n",
        "* Split: **exactly 9,000 train / 1,000 validation**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Data preprocessing and augmentation\n",
        "\n",
        "You must implement a **PyTorch `Dataset` class** that:\n",
        "\n",
        "* Reads `labels.csv`\n",
        "* Loads the corresponding image (from `data/`)\n",
        "* Returns both:\n",
        "  * the image (as a tensor)\n",
        "  * the labels (counts for 6 shapes)\n",
        "* Optionally applies transformations\n",
        "\n",
        "### Required augmentations\n",
        "\n",
        "You must implement **at least three** of the following:\n",
        "\n",
        "1. Random horizontal flip\n",
        "2. Random vertical flip\n",
        "3. Random 90° rotation (must correctly rotate orientation labels: up → right → down → left)\n",
        "4. Random brightness/contrast (mild)\n",
        "5. Gaussian noise\n",
        "6. Random erasing (small areas only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation metrics\n",
        "\n",
        "Implement and report the following metrics on the validation set:\n",
        "\n",
        "### (a) **Classification (135-way)**\n",
        "\n",
        "* Top-1 accuracy\n",
        "* Macro F1-score\n",
        "* Per-pair accuracy (aggregate by unordered shape pair, e.g. {circle, up})\n",
        "\n",
        "### (b) **Regression (6-D counts)**\n",
        "\n",
        "* RMSE per class and overall\n",
        "* MAE per class and overall\n",
        "\n",
        "Also plot:\n",
        "\n",
        "* Training and validation losses\n",
        "* Validation accuracy and RMSE over epochs\n",
        "\n",
        "**Important**: This task is not about finding the best architecture; we expect at least 50% accuracy, but achieving results higher than that will not affect the grade for the assignment**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Experiments and analysis\n",
        "\n",
        "You must train and compare **three model settings**:\n",
        "\n",
        "| Setting | Description                                      |\n",
        "| :------ | :----------------------------------------------- |\n",
        "| 1       | **Classification-only:** λ_cnt = 0               |\n",
        "| 2       | **Regression-only:** classification loss ignored |\n",
        "| 3       | **Multitask:** λ_cnt = with your choose          |\n",
        "\n",
        "For each experiment:\n",
        "\n",
        "* Train until early stopping\n",
        "* Record loss, accuracy, RMSE, and runtime\n",
        "* Compare results and explain how λ influences learning\n",
        "* Discuss whether multitask learning improves the main tasks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Final deliverables\n",
        "\n",
        "You must submit .zip project with:\n",
        "\n",
        "1. **Code** (`.ipynb` or `.py`) that:\n",
        "\n",
        "   * Downloads and extracts the dataset\n",
        "   * Defines dataset, dataloaders, model, loss, training loop, evaluation, and plotting\n",
        "   * Can run start-to-end without interaction, and finishes within 30 minutes on Colab T4 GPUs\n",
        "   * Includes three experiment configurations\n",
        "\n",
        "2. **Report (2–4 pages, PDF)** including:\n",
        "   * Section on (EDA) Exploratory Data Analysis in your report: no more than 3 graphs or tables describing the data set.\n",
        "   * Model architecture\n",
        "   * Description and justification of augmentations\n",
        "   * Results table (loss, accuracy, RMSE for all runs)\n",
        "   * Learning curves\n",
        "   * Discussion on multitask effects\n",
        "\n",
        "3. **README.md**:\n",
        "\n",
        "   * Link to Colab version of task for fast replication.\n",
        "   * Approximate runtime and resource requirements\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Grading rubric\n",
        "\n",
        "Component\tDescription\tPoints\n",
        "1. Implementation correctness\tCorrect use of the fixed backbone, two-headed model, and proper training loop (classification + regression).\t30%\n",
        "2. Data & augmentations\tProper dataset loading, preprocessing, and at least three augmentations with brief justification.\t20%\n",
        "3. Evaluation & experiments\tCorrect computation of metrics (accuracy, F1, RMSE) and completion of all three λ configurations (λ=0, regression-only, your choice λ).\t30%\n",
        "4. Report & analysis\n",
        "A clear separation of concerns (e.g. headers in notebooks, modules in code) and concise 2–4 page report with results tables, learning curves, confusion matrix, and short discussion on multitask effects and error examples.\n",
        "20%\n",
        "\n",
        "###### Readability and modularity will be considered within each grading component. Clear structure (headers in notebooks, docstrings, modular code) significantly improves evaluation speed. Emphasize using clear headers to help reviewers navigate efficiently.\n",
        "---"
      ],
      "metadata": {
        "id": "_NvRrg8YvTPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmggC-uvcGR",
        "outputId": "1da02c67-3a61-41f2-cb5b-5fb6a06ad82c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-10 13:21:43--  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip [following]\n",
            "--2025-11-10 13:21:44--  https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5544261 (5.3M) [application/zip]\n",
            "Saving to: ‘data_gsn.zip’\n",
            "\n",
            "data_gsn.zip        100%[===================>]   5.29M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-10 13:21:44 (115 MB/s) - ‘data_gsn.zip’ saved [5544261/5544261]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating labels for classification problem\n",
        "\n",
        "There are 135 combinations. This comes from:\n",
        "$$\\binom{6}{2}\\cdot 9 = 15\\cdot9 = 135$$\n",
        "\n",
        "We propose the following labeling: <br>\n",
        "For counting label $$[a_0, a_1, a_2, a_3, a_4, a_5]$$ with the only non-zero elements $a_i$ and $a_j$, where $i<j$ we will use a label equal to:\n",
        "$$ f(i, j)\\cdot9+a_i-1$$\n",
        "Where $f(g, h)$, where $g<h$, is a simple indexing function: <br>\n",
        "$$f(g,h) = \\binom{h}{2}+g$$ where $0\\leq g<h\\leq5$. This provides us with unique labels from $0$ to $134$"
      ],
      "metadata": {
        "id": "clsXRR2XUjf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math as mth\n",
        "\n",
        "def cnt_to_cls_idx(cnt_label):\n",
        "  a_1, i_1, i_2 = 0, 0, 0\n",
        "  for i, cnt in enumerate(cnt_label):\n",
        "    if cnt > 0:\n",
        "      if not a_1:\n",
        "        a_1 = cnt\n",
        "        i_1 = i\n",
        "      else:\n",
        "        i_2 = i\n",
        "        break\n",
        "  cls_idx = (mth.comb(i_2, 2)+i_1)*9+a_1-1\n",
        "  return cls_idx\n",
        "\n",
        "def cls_idx_to_cnt(cls_idx):\n",
        "    if not (0 <= cls_idx <= 134):\n",
        "        raise ValueError(\"cls_idx must be in [0, 134] for 6 labels and counts 1..9\")\n",
        "\n",
        "    a_1 = (cls_idx % 9) + 1\n",
        "    pair_idx = cls_idx // 9\n",
        "    i_2 = 1\n",
        "    while mth.comb(i_2 + 1, 2) <= pair_idx:\n",
        "        i_2 += 1\n",
        "    i_1 = pair_idx - mth.comb(i_2, 2)\n",
        "\n",
        "    cnt_label = [0]*6\n",
        "    cnt_label[i_1] = a_1\n",
        "    cnt_label[i_2] = 10 - a_1\n",
        "    return cnt_label\n",
        "\n",
        "\n",
        "# Test the labeling and create O(1) mapping\n",
        "labels = set()\n",
        "for a1 in range(1, 10):\n",
        "  for idx1 in range(6):\n",
        "    for idx2 in range(idx1+1, 6):\n",
        "      label_cnt = [0]*6\n",
        "      label_cnt[idx1] = a1\n",
        "      label_cnt[idx2] = 10-a1\n",
        "      label = cnt_to_cls_idx(label_cnt)\n",
        "      if label in labels:\n",
        "        print(f\"Not ok {label}\")\n",
        "      if cls_idx_to_cnt(label) != label_cnt:\n",
        "        print(\"Not ok\")\n",
        "      labels.add(label)\n",
        "\n",
        "if len(labels) == 135:\n",
        "  print(\"Ok\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BSqeLuQ1Uqpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b27ae6-da91-4b25-8feb-256b9f17b4c8"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding new labels"
      ],
      "metadata": {
        "id": "xs6a3_uSYYNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CSV_PATH = \"./data/labels.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "label_cols = df.columns[1:]\n",
        "n_images = len(df)\n",
        "\n",
        "class_indices = []\n",
        "for i in range(n_images):\n",
        "    cnt_label = df[label_cols].iloc[i].values\n",
        "    class_idx = cnt_to_cls_idx(cnt_label)\n",
        "    class_indices.append(class_idx)\n",
        "\n",
        "df[\"class_idx\"] = class_indices\n"
      ],
      "metadata": {
        "id": "eAGB4xBvYcg3"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Data\n"
      ],
      "metadata": {
        "id": "rhOb2Wyoyfer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "cnt_label_cols = df.columns[1:7]\n",
        "n_images = len(df)\n",
        "\n",
        "\n",
        "# --- COUNT LABELS ANALYSIS ---\n",
        "total_shapes     = df[cnt_label_cols].sum().sum()\n",
        "shape_totals     = df[cnt_label_cols].sum()\n",
        "shape_percentage = 100 * shape_totals / total_shapes\n",
        "\n",
        "corr = df[cnt_label_cols].corr(numeric_only=True)\n",
        "nonzero_means = df[cnt_label_cols].replace(0, np.nan).mean()\n",
        "presence_percentage = 100 * (df[cnt_label_cols] > 0).sum() / n_images\n",
        "\n",
        "# Per-image constraints\n",
        "nonzero_per_image = (df[cnt_label_cols] > 0).sum(axis=1)\n",
        "sum_per_image     = df[cnt_label_cols].sum(axis=1)\n",
        "\n",
        "prop_exact_two_nonzero = (nonzero_per_image == 2).mean() * 100\n",
        "prop_sum_eq_10         = (sum_per_image == 10).mean() * 100\n",
        "prop_both_constraints  = ((nonzero_per_image == 2) & (sum_per_image == 10)).mean() * 100\n",
        "\n",
        "# --- CLASS INDEX ANALYSIS (robust) ---\n",
        "n_classes = 135\n",
        "\n",
        "# counts for all classes [0..n_classes-1], missing -> 0\n",
        "class_counts_raw = df[\"class_idx\"].value_counts()          # counts only for seen classes\n",
        "class_counts_full = class_counts_raw.reindex(range(n_classes), fill_value=0)\n",
        "\n",
        "# summary table: count + % of images containing that class\n",
        "class_presence_pct = 100 * class_counts_full / n_images\n",
        "class_summary = pd.DataFrame({\n",
        "    \"class_id\": range(n_classes),\n",
        "    \"count\": class_counts_full.values,\n",
        "    \"pct_of_images\": class_presence_pct.values\n",
        "})\n",
        "\n",
        "# stats\n",
        "num_used_classes = int((class_counts_full > 0).sum())\n",
        "pct_classes_used = 100 * num_used_classes / n_classes\n",
        "unused_classes = class_summary.loc[class_summary[\"count\"] == 0, \"class_id\"].tolist()\n",
        "\n",
        "# top / bottom 10 (including unseen)\n",
        "top10_classes = class_summary.sort_values(\"count\", ascending=False).head(10)\n",
        "least10_present = (\n",
        "    class_summary[class_summary[\"count\"] > 0]\n",
        "    .sort_values([\"count\", \"class_id\"], ascending=[True, True])\n",
        "    .head(10)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "# --- PRINT SUMMARY ---\n",
        "print(\"=== DATA OVERVIEW ===\")\n",
        "print(f\"Images: {n_images}\")\n",
        "print(f\"Count label columns: {list(label_cols)}\")\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "print(\"\\n=== COUNT LABELS SUMMARY ===\")\n",
        "print(f\"Total sum of all counts: {int(total_shapes)}\")\n",
        "print(\"\\nPer-label totals:\")\n",
        "print(shape_totals)\n",
        "print(\"\\nPer-label percentage of total (%):\")\n",
        "print(shape_percentage.round(2))\n",
        "print(\"\\nPresence percentage per label (%% of images with >0):\")\n",
        "print(presence_percentage.round(2))\n",
        "print(\"\\nMean of non-zero values per label:\")\n",
        "print(nonzero_means.round(3))\n",
        "\n",
        "print(\"\\n=== CONSTRAINT CHECKS ===\")\n",
        "print(f\"Images with exactly 2 non-zero count labels: {prop_exact_two_nonzero:.2f}%\")\n",
        "print(f\"Images with sum(counts) == 10: {prop_sum_eq_10:.2f}%\")\n",
        "print(f\"Images satisfying both: {prop_both_constraints:.2f}%\")\n",
        "\n",
        "print(\"\\nCorrelation matrix between count labels:\")\n",
        "print(corr.round(3))\n",
        "\n",
        "print(f\"Classes used: {num_used_classes}/{n_classes} ({pct_classes_used:.2f}%)\")\n",
        "print(\"Top 10 classes (id, count, % of images):\")\n",
        "print(top10_classes[[\"class_id\", \"count\", \"pct_of_images\"]].to_string(index=False))\n",
        "\n",
        "print(\"\\nLeast 10 classes (id, count, % of images):\")\n",
        "print(least10_present[[\"class_id\", \"count\", \"pct_of_images\"]].to_string(index=False))\n",
        "print(\"Unused common classes:\")\n",
        "print(unused_classes)\n"
      ],
      "metadata": {
        "id": "KbjAlQiVyjY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7510d536-b7bb-4578-c6ff-5a2e733090f9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA OVERVIEW ===\n",
            "Images: 10000\n",
            "Count label columns: ['squares', 'circles', 'up', 'right', 'down', 'left']\n",
            "Number of classes: 135\n",
            "\n",
            "=== COUNT LABELS SUMMARY ===\n",
            "Total sum of all counts: 100000\n",
            "\n",
            "Per-label totals:\n",
            "squares    16574\n",
            "circles    17149\n",
            "up         16530\n",
            "right      16770\n",
            "down       16857\n",
            "left       16120\n",
            "dtype: int64\n",
            "\n",
            "Per-label percentage of total (%):\n",
            "squares    16.57\n",
            "circles    17.15\n",
            "up         16.53\n",
            "right      16.77\n",
            "down       16.86\n",
            "left       16.12\n",
            "dtype: float64\n",
            "\n",
            "Presence percentage per label (%% of images with >0):\n",
            "squares    32.79\n",
            "circles    34.10\n",
            "up         33.19\n",
            "right      33.54\n",
            "down       33.67\n",
            "left       32.71\n",
            "dtype: float64\n",
            "\n",
            "Mean of non-zero values per label:\n",
            "squares    5.055\n",
            "circles    5.029\n",
            "up         4.980\n",
            "right      5.000\n",
            "down       5.007\n",
            "left       4.928\n",
            "dtype: float64\n",
            "\n",
            "=== CONSTRAINT CHECKS ===\n",
            "Images with exactly 2 non-zero count labels: 100.00%\n",
            "Images with sum(counts) == 10: 100.00%\n",
            "Images satisfying both: 100.00%\n",
            "\n",
            "Correlation matrix between count labels:\n",
            "         squares  circles     up  right   down   left\n",
            "squares    1.000   -0.202 -0.202 -0.206 -0.200 -0.192\n",
            "circles   -0.202    1.000 -0.209 -0.201 -0.208 -0.195\n",
            "up        -0.202   -0.209  1.000 -0.191 -0.197 -0.198\n",
            "right     -0.206   -0.201 -0.191  1.000 -0.207 -0.197\n",
            "down      -0.200   -0.208 -0.197 -0.207  1.000 -0.196\n",
            "left      -0.192   -0.195 -0.198 -0.197 -0.196  1.000\n",
            "Classes used: 105/135 (77.78%)\n",
            "Top 10 classes (id, count, % of images):\n",
            " class_id  count  pct_of_images\n",
            "       41    120           1.20\n",
            "       74    115           1.15\n",
            "       79    112           1.12\n",
            "       52    112           1.12\n",
            "       49    109           1.09\n",
            "        5    108           1.08\n",
            "      121    107           1.07\n",
            "      103    107           1.07\n",
            "       32    107           1.07\n",
            "       19    106           1.06\n",
            "\n",
            "Least 10 classes (id, count, % of images):\n",
            " class_id  count  pct_of_images\n",
            "       91     58           0.58\n",
            "       10     75           0.75\n",
            "       33     78           0.78\n",
            "      112     79           0.79\n",
            "      120     79           0.79\n",
            "       21     80           0.80\n",
            "       23     80           0.80\n",
            "       84     81           0.81\n",
            "      118     81           0.81\n",
            "       76     82           0.82\n",
            "Unused common classes:\n",
            "[0, 8, 9, 17, 18, 26, 27, 35, 36, 44, 45, 53, 54, 62, 63, 71, 72, 80, 81, 89, 90, 98, 99, 107, 108, 116, 117, 125, 126, 134]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "The dataset is well balanced: all shapes appear with similar frequency, there are no strong correlations between shape types, and whenever a shape is present its average count is close to 5 out of 10. Based on this, not much data augmentation is needed."
      ],
      "metadata": {
        "id": "eVQLy-7S7_WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class\n",
        "Each transform is applied independently with a fixed probability."
      ],
      "metadata": {
        "id": "L4QwKRS1uwnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ake3FpnUpxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "class MultitaskDataset(Dataset):\n",
        "\n",
        "  def __init__(self, root: str, transforms = None, transform_prob: float = 0.3):\n",
        "    self.root = Path(root)\n",
        "    self.df = pd.read_csv(root)\n",
        "    if transforms is None:\n",
        "      transforms = []\n",
        "    self.transforms = transforms\n",
        "    self.transform_prob = transform_prob\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.root.parent / self.df.iloc[idx, 0]\n",
        "    image = Image.open(img_path).convert('L')\n",
        "    label_cnt = self.df.iloc[idx, 1:].tolist()\n",
        "\n",
        "\n",
        "\n",
        "    for transform in self.transforms:\n",
        "      if torch.rand(1) < self.transform_prob:\n",
        "        image, labels_cnt = transform(image, label_cnt)\n",
        "\n",
        "\n",
        "    cls_idx = cnt_to_cls_idx(label_cnt)\n",
        "    image = TF.to_tensor(image)\n",
        "    return image, torch.tensor(label_cnt, dtype=torch.float32), cls_idx\n",
        "\n",
        "\n",
        "def horizontal_flip(image, labels_cnt):\n",
        "  labels_cnt[3], labels_cnt[5] = labels_cnt[5], labels_cnt[3]\n",
        "  return image.transpose(Image.FLIP_LEFT_RIGHT), labels_cnt\n",
        "\n",
        "\n",
        "def vertical_flip(image, labels_cnt):\n",
        "  labels_cnt[2], labels_cnt[4] = labels_cnt[4], labels_cnt[2]\n",
        "  return image.transpose(Image.FLIP_TOP_BOTTOM), labels_cnt\n",
        "\n",
        "def rotation(image, labels_cnt):\n",
        "  labels_cnt = [labels_cnt[0], labels_cnt[1], labels_cnt[5], labels_cnt[2], labels_cnt[3], labels_cnt[4]]\n",
        "  return image.rotate(90), labels_cnt\n",
        "\n"
      ],
      "metadata": {
        "id": "Noooak7Y18Bx"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the data"
      ],
      "metadata": {
        "id": "vE1fwqZovBc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "g = torch.Generator().manual_seed(1)\n",
        "\n",
        "train_base = MultitaskDataset(\n",
        "    root=\"./data/labels.csv\",\n",
        "    transforms=[horizontal_flip, vertical_flip, rotation],\n",
        "    transform_prob=0.3\n",
        ")\n",
        "\n",
        "test_base = MultitaskDataset(\n",
        "    root=\"./data/labels.csv\",\n",
        "    transforms=[],\n",
        ")\n",
        "\n",
        "n = len(train_base)\n",
        "split = int(0.9 * n)\n",
        "\n",
        "train_idx = [i for i in range(split)]\n",
        "test_idx  = [i for i in range(split,n)]\n",
        "\n",
        "assert (len(train_idx), len(test_idx)) == (9000, 1000)\n",
        "\n",
        "train_ds = Subset(train_base, train_idx)\n",
        "test_ds  = Subset(test_base,  test_idx)\n",
        "\n",
        "num_workers = min(4, os.cpu_count() // 2)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "uDskVAXfvkFn"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network structure\n"
      ],
      "metadata": {
        "id": "_ObxxKDb9YDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        "        )\n",
        "        self.head_cls = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 135)\n",
        "        )\n",
        "        self.head_cnt = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 6)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        logits = self.head_cls(feat)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        counts = self.head_cnt(feat)\n",
        "        return log_probs, counts\n"
      ],
      "metadata": {
        "id": "Ps2PbavO95CD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n"
      ],
      "metadata": {
        "id": "D32dQyZbHAhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualiser"
      ],
      "metadata": {
        "id": "y8mlBVi99dzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class TrainingVisualizer:\n",
        "#     def __init__(self, lr):\n",
        "#         self.lr = lr\n",
        "#         self.first_linear_layer = None\n",
        "\n",
        "#     def attach(self, model: nn.Module):\n",
        "#         \"\"\"Znajdź pierwszą warstwę liniową w modelu i zapamiętaj referencję.\"\"\"\n",
        "#         self.first_linear_layer = None\n",
        "#         for m in model.modules():\n",
        "#             if isinstance(m, nn.Linear):\n",
        "#                 self.first_linear_layer = m\n",
        "#                 break\n",
        "\n",
        "#     def plot_gradients_and_loss(self, batch_idx: int, loss: float):\n",
        "#         layer = self.first_linear_layer\n",
        "#         # Jeśli nie znaleziono warstwy lub brak gradientu – po prostu pomiń\n",
        "#         if layer is None or layer.weight is None or layer.weight.grad is None:\n",
        "#             return\n",
        "\n",
        "#         # Zabezpieczenia przed dzieleniem przez zero / NaN\n",
        "#         w_std = layer.weight.detach().std()\n",
        "#         g_std = layer.weight.grad.detach().std()\n",
        "#         if w_std == 0 or torch.isnan(w_std) or torch.isnan(g_std):\n",
        "#             return\n",
        "\n",
        "#         grad_to_weight_ratio = (self.lr * g_std / w_std).item()\n",
        "\n",
        "#         # ...tu Twoje rysowanie / logowanie:\n",
        "#         # self.writer.add_scalar(\"train/loss\", loss, batch_idx)\n",
        "#         # self.writer.add_scalar(\"train/grad_to_weight_ratio\", grad_to_weight_ratio, batch_idx)\n"
      ],
      "metadata": {
        "id": "ria9VZF0G6k1"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "MnAgvVKy9i7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    device: torch.device,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epoch: int,\n",
        "    log_interval: int,\n",
        "    # visualizer: TrainingVisualizer,\n",
        "    verbose: bool = False,\n",
        "    lambda_cnt: float = 0.5\n",
        ") -> None:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (data, target_cnt, target_cls) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "        data = data.to(device)\n",
        "        target_cls = target_cls.to(device).long()\n",
        "        target_cnt = target_cnt.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        log_probs, counts = model(data)\n",
        "\n",
        "        loss_cls = F.nll_loss(log_probs, target_cls)\n",
        "        loss_cnt = F.smooth_l1_loss(counts, target_cnt)\n",
        "\n",
        "        loss = loss_cls + lambda_cnt * loss_cnt\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # visualizer.plot_gradients_and_loss(batch_idx, loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            if verbose:\n",
        "                done, total = batch_idx * len(data), len(train_loader.dataset)\n",
        "                print(\n",
        "                    f\"Train Epoch: {epoch} [{done}/{total} images ({done / total:.0%})]\\t\"\n",
        "                    + f\"Loss: {loss.item():.6f}\"\n",
        "                )\n",
        "\n"
      ],
      "metadata": {
        "id": "nM1ILMQ07fTq"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(\n",
        "    model: torch.nn.Module,\n",
        "    device: torch.device,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    epoch: int,\n",
        "    # visualizer: TrainingVisualizer,\n",
        "    verbose: bool = False,\n",
        ") -> None:\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  n_correct_cnt = 0\n",
        "  n_correct_cls = 0\n",
        "  n_total = 0\n",
        "  total_loss_cls = 0\n",
        "  total_loss_cnt = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target_cnt, target_cls in test_loader:\n",
        "          data = data.to(device)\n",
        "          target_cls = target_cls.to(device).long()\n",
        "          target_cnt = target_cnt.to(device).float()\n",
        "\n",
        "          log_probs, counts = model(data)\n",
        "\n",
        "          loss_cls = F.nll_loss(log_probs, target_cls, reduction=\"sum\").item()\n",
        "          loss_cnt = F.smooth_l1_loss(counts, target_cnt, reduction=\"sum\").item()\n",
        "\n",
        "          total_loss_cls += loss_cls\n",
        "          total_loss_cnt += loss_cnt\n",
        "\n",
        "          pred_cls = log_probs.argmax(dim=1)\n",
        "          n_correct_cls += pred_cls.eq(target_cls).sum().item()\n",
        "          n_correct_cnt += (counts.round() == target_cnt).all(dim=1).sum().item()\n",
        "\n",
        "          n_total += len(data)\n",
        "\n",
        "  test_loss_cls = total_loss_cls / n_total\n",
        "  test_loss_cnt = total_loss_cnt / n_total\n",
        "\n",
        "    # === PRINT METRICS ===\n",
        "  # === PRINT METRICS ===\n",
        "  if verbose:\n",
        "      print(f\"\\nTest set:\")\n",
        "      print(f\"  Classification (135-way):\")\n",
        "      print(f\"    Top-1 accuracy: {100.0 * n_correct_cls / n_total:.2f}%\")\n",
        "      print(f\"    Per-pair accuracy (aggregate by unordered shape pair): {100.0 * n_correct_cnt / n_total:.2f}%\")\n",
        "\n",
        "      print(f\"\\n  Regression (6-D counts):\")\n",
        "      rmse_overall = (total_loss_cnt / n_total) ** 0.5\n",
        "      mae_overall = total_loss_cnt / n_total\n",
        "      print(f\"    RMSE overall: {rmse_overall:.4f}\")\n",
        "      print(f\"    MAE overall: {mae_overall:.4f}\")\n",
        "\n",
        "      print(f\"\\n  Losses: cls={total_loss_cls / n_total:.4f}, cnt={total_loss_cnt / n_total:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "FwU7CYdY1h4C"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "Kg_dRGdo3D44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lambda_cnt = 0.0\n",
        "lr = 0.001\n",
        "\n",
        "log_interval = 10\n"
      ],
      "metadata": {
        "id": "kYxMHoc43HRA"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "yFr2cj6W5M74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "model = MultiTaskNet().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# visualizer = TrainingVisualizer(lr=optimizer.param_groups[0][\"lr\"])\n",
        "# visualizer.attach(model)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_epoch(\n",
        "        model,\n",
        "        device,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        epoch,\n",
        "        log_interval,\n",
        "        # visualizer,\n",
        "        verbose=True,\n",
        "        lambda_cnt=lambda_cnt\n",
        "    )\n",
        "    test(model, device, test_loader, epoch, verbose = True) #visualizer, verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "GnzDL1pSAot5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad69784-2105-47bc-9128-a2234f514987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/9000 images (0%)]\tLoss: 4.912560\n",
            "Train Epoch: 1 [640/9000 images (7%)]\tLoss: 4.862555\n",
            "Train Epoch: 1 [1280/9000 images (14%)]\tLoss: 4.794497\n",
            "Train Epoch: 1 [1920/9000 images (21%)]\tLoss: 4.755055\n",
            "Train Epoch: 1 [2560/9000 images (28%)]\tLoss: 4.647931\n",
            "Train Epoch: 1 [3200/9000 images (36%)]\tLoss: 4.762452\n",
            "Train Epoch: 1 [3840/9000 images (43%)]\tLoss: 4.677688\n",
            "Train Epoch: 1 [4480/9000 images (50%)]\tLoss: 4.672613\n",
            "Train Epoch: 1 [5120/9000 images (57%)]\tLoss: 4.701868\n",
            "Train Epoch: 1 [5760/9000 images (64%)]\tLoss: 4.706580\n",
            "Train Epoch: 1 [6400/9000 images (71%)]\tLoss: 4.689613\n",
            "Train Epoch: 1 [7040/9000 images (78%)]\tLoss: 4.640472\n",
            "Train Epoch: 1 [7680/9000 images (85%)]\tLoss: 4.689623\n",
            "Train Epoch: 1 [8320/9000 images (92%)]\tLoss: 4.713764\n",
            "Train Epoch: 1 [5600/9000 images (62%)]\tLoss: 4.641328\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 0.90%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 3.0015\n",
            "    MAE overall: 9.0089\n",
            "\n",
            "  Losses: cls=4.6775, cnt=9.0089\n",
            "\n",
            "Train Epoch: 2 [0/9000 images (0%)]\tLoss: 4.639828\n",
            "Train Epoch: 2 [640/9000 images (7%)]\tLoss: 4.666946\n",
            "Train Epoch: 2 [1280/9000 images (14%)]\tLoss: 4.694525\n",
            "Train Epoch: 2 [1920/9000 images (21%)]\tLoss: 4.666007\n",
            "Train Epoch: 2 [2560/9000 images (28%)]\tLoss: 4.653529\n",
            "Train Epoch: 2 [3200/9000 images (36%)]\tLoss: 4.669426\n",
            "Train Epoch: 2 [3840/9000 images (43%)]\tLoss: 4.684622\n",
            "Train Epoch: 2 [4480/9000 images (50%)]\tLoss: 4.668679\n",
            "Train Epoch: 2 [5120/9000 images (57%)]\tLoss: 4.664980\n",
            "Train Epoch: 2 [5760/9000 images (64%)]\tLoss: 4.662996\n",
            "Train Epoch: 2 [6400/9000 images (71%)]\tLoss: 4.636814\n",
            "Train Epoch: 2 [7040/9000 images (78%)]\tLoss: 4.658535\n",
            "Train Epoch: 2 [7680/9000 images (85%)]\tLoss: 4.665946\n",
            "Train Epoch: 2 [8320/9000 images (92%)]\tLoss: 4.640697\n",
            "Train Epoch: 2 [5600/9000 images (62%)]\tLoss: 4.674028\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 0.70%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9886\n",
            "    MAE overall: 8.9318\n",
            "\n",
            "  Losses: cls=4.6693, cnt=8.9318\n",
            "\n",
            "Train Epoch: 3 [0/9000 images (0%)]\tLoss: 4.660936\n",
            "Train Epoch: 3 [640/9000 images (7%)]\tLoss: 4.617527\n",
            "Train Epoch: 3 [1280/9000 images (14%)]\tLoss: 4.606472\n",
            "Train Epoch: 3 [1920/9000 images (21%)]\tLoss: 4.577536\n",
            "Train Epoch: 3 [2560/9000 images (28%)]\tLoss: 4.588314\n",
            "Train Epoch: 3 [3200/9000 images (36%)]\tLoss: 4.489487\n",
            "Train Epoch: 3 [3840/9000 images (43%)]\tLoss: 4.440985\n",
            "Train Epoch: 3 [4480/9000 images (50%)]\tLoss: 4.361297\n",
            "Train Epoch: 3 [5120/9000 images (57%)]\tLoss: 4.411517\n",
            "Train Epoch: 3 [5760/9000 images (64%)]\tLoss: 4.398129\n",
            "Train Epoch: 3 [6400/9000 images (71%)]\tLoss: 4.341021\n",
            "Train Epoch: 3 [7040/9000 images (78%)]\tLoss: 4.430822\n",
            "Train Epoch: 3 [7680/9000 images (85%)]\tLoss: 4.347762\n",
            "Train Epoch: 3 [8320/9000 images (92%)]\tLoss: 4.390680\n",
            "Train Epoch: 3 [5600/9000 images (62%)]\tLoss: 4.365019\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 3.00%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9784\n",
            "    MAE overall: 8.8706\n",
            "\n",
            "  Losses: cls=4.1741, cnt=8.8706\n",
            "\n",
            "Train Epoch: 4 [0/9000 images (0%)]\tLoss: 4.279154\n",
            "Train Epoch: 4 [640/9000 images (7%)]\tLoss: 4.343098\n",
            "Train Epoch: 4 [1280/9000 images (14%)]\tLoss: 4.272136\n",
            "Train Epoch: 4 [1920/9000 images (21%)]\tLoss: 4.184206\n",
            "Train Epoch: 4 [2560/9000 images (28%)]\tLoss: 4.233474\n",
            "Train Epoch: 4 [3200/9000 images (36%)]\tLoss: 4.323458\n",
            "Train Epoch: 4 [3840/9000 images (43%)]\tLoss: 4.346083\n",
            "Train Epoch: 4 [4480/9000 images (50%)]\tLoss: 4.242755\n",
            "Train Epoch: 4 [5120/9000 images (57%)]\tLoss: 4.171920\n",
            "Train Epoch: 4 [5760/9000 images (64%)]\tLoss: 4.037695\n",
            "Train Epoch: 4 [6400/9000 images (71%)]\tLoss: 4.074749\n",
            "Train Epoch: 4 [7040/9000 images (78%)]\tLoss: 4.043154\n",
            "Train Epoch: 4 [7680/9000 images (85%)]\tLoss: 3.853912\n",
            "Train Epoch: 4 [8320/9000 images (92%)]\tLoss: 3.975145\n",
            "Train Epoch: 4 [5600/9000 images (62%)]\tLoss: 3.945962\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 7.10%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9535\n",
            "    MAE overall: 8.7229\n",
            "\n",
            "  Losses: cls=3.5411, cnt=8.7229\n",
            "\n",
            "Train Epoch: 5 [0/9000 images (0%)]\tLoss: 3.996071\n",
            "Train Epoch: 5 [640/9000 images (7%)]\tLoss: 3.985494\n",
            "Train Epoch: 5 [1280/9000 images (14%)]\tLoss: 3.793936\n",
            "Train Epoch: 5 [1920/9000 images (21%)]\tLoss: 3.746459\n",
            "Train Epoch: 5 [2560/9000 images (28%)]\tLoss: 3.631456\n",
            "Train Epoch: 5 [3200/9000 images (36%)]\tLoss: 3.705022\n",
            "Train Epoch: 5 [3840/9000 images (43%)]\tLoss: 3.710748\n",
            "Train Epoch: 5 [4480/9000 images (50%)]\tLoss: 3.842229\n",
            "Train Epoch: 5 [5120/9000 images (57%)]\tLoss: 3.621679\n",
            "Train Epoch: 5 [5760/9000 images (64%)]\tLoss: 3.558862\n",
            "Train Epoch: 5 [6400/9000 images (71%)]\tLoss: 3.560452\n",
            "Train Epoch: 5 [7040/9000 images (78%)]\tLoss: 3.601445\n",
            "Train Epoch: 5 [7680/9000 images (85%)]\tLoss: 3.382089\n",
            "Train Epoch: 5 [8320/9000 images (92%)]\tLoss: 3.354013\n",
            "Train Epoch: 5 [5600/9000 images (62%)]\tLoss: 3.341500\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 14.20%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9445\n",
            "    MAE overall: 8.6700\n",
            "\n",
            "  Losses: cls=2.9735, cnt=8.6700\n",
            "\n",
            "Train Epoch: 6 [0/9000 images (0%)]\tLoss: 3.164466\n",
            "Train Epoch: 6 [640/9000 images (7%)]\tLoss: 3.330350\n",
            "Train Epoch: 6 [1280/9000 images (14%)]\tLoss: 3.017136\n",
            "Train Epoch: 6 [1920/9000 images (21%)]\tLoss: 3.111064\n",
            "Train Epoch: 6 [2560/9000 images (28%)]\tLoss: 2.994281\n",
            "Train Epoch: 6 [3200/9000 images (36%)]\tLoss: 2.965327\n",
            "Train Epoch: 6 [3840/9000 images (43%)]\tLoss: 3.230411\n",
            "Train Epoch: 6 [4480/9000 images (50%)]\tLoss: 3.077350\n",
            "Train Epoch: 6 [5120/9000 images (57%)]\tLoss: 2.961676\n",
            "Train Epoch: 6 [5760/9000 images (64%)]\tLoss: 2.775094\n",
            "Train Epoch: 6 [6400/9000 images (71%)]\tLoss: 3.012428\n",
            "Train Epoch: 6 [7040/9000 images (78%)]\tLoss: 3.036455\n",
            "Train Epoch: 6 [7680/9000 images (85%)]\tLoss: 3.108243\n",
            "Train Epoch: 6 [8320/9000 images (92%)]\tLoss: 2.692343\n",
            "Train Epoch: 6 [5600/9000 images (62%)]\tLoss: 3.205762\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 21.30%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9532\n",
            "    MAE overall: 8.7217\n",
            "\n",
            "  Losses: cls=2.5947, cnt=8.7217\n",
            "\n",
            "Train Epoch: 7 [0/9000 images (0%)]\tLoss: 2.878059\n",
            "Train Epoch: 7 [640/9000 images (7%)]\tLoss: 2.779571\n",
            "Train Epoch: 7 [1280/9000 images (14%)]\tLoss: 3.194326\n",
            "Train Epoch: 7 [1920/9000 images (21%)]\tLoss: 2.947349\n",
            "Train Epoch: 7 [2560/9000 images (28%)]\tLoss: 3.057205\n",
            "Train Epoch: 7 [3200/9000 images (36%)]\tLoss: 2.665315\n",
            "Train Epoch: 7 [3840/9000 images (43%)]\tLoss: 2.723911\n",
            "Train Epoch: 7 [4480/9000 images (50%)]\tLoss: 2.883091\n",
            "Train Epoch: 7 [5120/9000 images (57%)]\tLoss: 2.640624\n",
            "Train Epoch: 7 [5760/9000 images (64%)]\tLoss: 2.656798\n",
            "Train Epoch: 7 [6400/9000 images (71%)]\tLoss: 2.924452\n",
            "Train Epoch: 7 [7040/9000 images (78%)]\tLoss: 2.821607\n",
            "Train Epoch: 7 [7680/9000 images (85%)]\tLoss: 2.532358\n",
            "Train Epoch: 7 [8320/9000 images (92%)]\tLoss: 2.804681\n",
            "Train Epoch: 7 [5600/9000 images (62%)]\tLoss: 2.768266\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 27.50%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9644\n",
            "    MAE overall: 8.7879\n",
            "\n",
            "  Losses: cls=2.3239, cnt=8.7879\n",
            "\n",
            "Train Epoch: 8 [0/9000 images (0%)]\tLoss: 2.477784\n",
            "Train Epoch: 8 [640/9000 images (7%)]\tLoss: 2.684251\n",
            "Train Epoch: 8 [1280/9000 images (14%)]\tLoss: 2.728942\n",
            "Train Epoch: 8 [1920/9000 images (21%)]\tLoss: 2.729006\n",
            "Train Epoch: 8 [2560/9000 images (28%)]\tLoss: 2.702575\n",
            "Train Epoch: 8 [3200/9000 images (36%)]\tLoss: 2.483379\n",
            "Train Epoch: 8 [3840/9000 images (43%)]\tLoss: 2.681739\n",
            "Train Epoch: 8 [4480/9000 images (50%)]\tLoss: 2.468562\n",
            "Train Epoch: 8 [5120/9000 images (57%)]\tLoss: 2.920843\n",
            "Train Epoch: 8 [5760/9000 images (64%)]\tLoss: 2.648393\n",
            "Train Epoch: 8 [6400/9000 images (71%)]\tLoss: 2.564841\n",
            "Train Epoch: 8 [7040/9000 images (78%)]\tLoss: 2.391889\n",
            "Train Epoch: 8 [7680/9000 images (85%)]\tLoss: 2.445586\n",
            "Train Epoch: 8 [8320/9000 images (92%)]\tLoss: 2.282814\n",
            "Train Epoch: 8 [5600/9000 images (62%)]\tLoss: 2.764392\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 30.60%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9793\n",
            "    MAE overall: 8.8760\n",
            "\n",
            "  Losses: cls=2.1205, cnt=8.8760\n",
            "\n",
            "Train Epoch: 9 [0/9000 images (0%)]\tLoss: 2.328585\n",
            "Train Epoch: 9 [640/9000 images (7%)]\tLoss: 2.324260\n",
            "Train Epoch: 9 [1280/9000 images (14%)]\tLoss: 2.397992\n",
            "Train Epoch: 9 [1920/9000 images (21%)]\tLoss: 2.601150\n",
            "Train Epoch: 9 [2560/9000 images (28%)]\tLoss: 2.446231\n",
            "Train Epoch: 9 [3200/9000 images (36%)]\tLoss: 2.493546\n",
            "Train Epoch: 9 [3840/9000 images (43%)]\tLoss: 2.331229\n",
            "Train Epoch: 9 [4480/9000 images (50%)]\tLoss: 2.729594\n",
            "Train Epoch: 9 [5120/9000 images (57%)]\tLoss: 2.408577\n",
            "Train Epoch: 9 [5760/9000 images (64%)]\tLoss: 2.281192\n",
            "Train Epoch: 9 [6400/9000 images (71%)]\tLoss: 2.433730\n",
            "Train Epoch: 9 [7040/9000 images (78%)]\tLoss: 2.307217\n",
            "Train Epoch: 9 [7680/9000 images (85%)]\tLoss: 2.337749\n",
            "Train Epoch: 9 [8320/9000 images (92%)]\tLoss: 2.319109\n",
            "Train Epoch: 9 [5600/9000 images (62%)]\tLoss: 2.404444\n",
            "\n",
            "Test set:\n",
            "  Classification (135-way):\n",
            "    Top-1 accuracy: 27.10%\n",
            "    Per-pair accuracy (aggregate by unordered shape pair): 0.00%\n",
            "\n",
            "  Regression (6-D counts):\n",
            "    RMSE overall: 2.9812\n",
            "    MAE overall: 8.8876\n",
            "\n",
            "  Losses: cls=2.1001, cnt=8.8876\n",
            "\n",
            "Train Epoch: 10 [0/9000 images (0%)]\tLoss: 2.442341\n",
            "Train Epoch: 10 [640/9000 images (7%)]\tLoss: 2.221484\n",
            "Train Epoch: 10 [1280/9000 images (14%)]\tLoss: 2.389421\n",
            "Train Epoch: 10 [1920/9000 images (21%)]\tLoss: 2.621212\n",
            "Train Epoch: 10 [2560/9000 images (28%)]\tLoss: 2.176527\n",
            "Train Epoch: 10 [3200/9000 images (36%)]\tLoss: 2.513270\n",
            "Train Epoch: 10 [3840/9000 images (43%)]\tLoss: 2.256407\n",
            "Train Epoch: 10 [4480/9000 images (50%)]\tLoss: 2.538076\n",
            "Train Epoch: 10 [5120/9000 images (57%)]\tLoss: 2.425772\n",
            "Train Epoch: 10 [5760/9000 images (64%)]\tLoss: 2.296877\n",
            "Train Epoch: 10 [6400/9000 images (71%)]\tLoss: 2.357135\n",
            "Train Epoch: 10 [7040/9000 images (78%)]\tLoss: 2.495024\n",
            "Train Epoch: 10 [7680/9000 images (85%)]\tLoss: 2.552728\n"
          ]
        }
      ]
    }
  ]
}